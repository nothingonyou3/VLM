# -*- coding: utf-8 -*-
"""4_CLASSES_QuiltNet_bach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OY811JAvI-3Q5bMyCmwCb_NEttkrW3--
"""

! pip install ftfy regex tqdm
! pip install git+https://github.com/openai/CLIP.git

! pip install open_clip_torch

from google.colab import drive
drive.mount('/content/drive')

import open_clip
import os
import clip
import torch
from PIL import Image

# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"
#model, preprocess = clip.load('ViT-B/32', device)

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:wisdomik/QuiltNet-B-16')
tokenizer = open_clip.get_tokenizer('hf-hub:wisdomik/QuiltNet-B-16')

#Data loading

image_folder = '/content/drive/MyDrive/Unique_BACH_folder'  # Specifica il percorso alla cartella delle immagini
image_files = [f for f in os.listdir(image_folder) if f.endswith(('tif'))]

import pandas as pd

# Leggi il file Excel
df = pd.read_csv('/content/drive/MyDrive/WE/microscopy_ground_truth.csv')

# Inizializza la lista per le coppie
y_vere = []

import numpy as np

y_pred = []

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)


templates = ["a histopathology slide showing {}",
"histopathology image of {}",
"pathology tissue showing {}",
"presence of {} tissue on image"]

classnames = ["Breast normal breast tissue", "Breast non-malignant benign tissue", "Breast malignant in-situ carcinoma", "Breast malignant invasive carcinoma"]

clip_weights = []
for classname in classnames:
    texts = []
    # Tokenize the prompts
    classname = classname.replace('_', ' ')
    for t in templates:
      texts.append(t.format(classname))

    text_tokens = clip.tokenize(texts).to(device)
    class_embeddings = model.encode_text(text_tokens)
    class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)
    class_embeddings /= class_embeddings.norm(dim=1, keepdim=True)
    clip_weights.append(class_embeddings)
print(len(clip_weights))
clip_weights = torch.stack(clip_weights, dim=-1).to(device)
print(clip_weights.shape)
text_features = clip_weights.mean(dim=0)
print(text_features.shape)

texts = ["Breast normal breast tissue", "Breast non-malignant benign tissue", "Breast malignant in-situ carcinoma", "Breast malignant invasive carcinoma"]
"""
with torch.no_grad():
  text = tokenizer(texts).to(device)
  text_features = model.encode_text(text)
  print(text_features.shape)
  text_features /= text_features.norm(dim=-1, keepdim=True)
  print("THE SHAAAAAAAAAAAAAAAAAAPE---------")
  print(text_features.shape)
"""
y_code = {
    "Normal": 0,
    "Benign": 1,
    "InSitu": 2,
    "Invasive": 3
}

all_pred = []
all_y = []

for index, row in df.iterrows():
    coppia = (row[0], row[1])  # Leggi la colonna 0 e la colonna 1 e crea la coppia
    image_file = row[0]
    yt = row[1]
    y_int = y_code[yt]

    image_path = os.path.join(image_folder, image_file)
    image = Image.open(image_path)

    # Preprocessa l'immagine usando le trasformazioni di validazione
    image_preprocessed = preprocess_val(image).unsqueeze(0)  # aggiungi una dimensione batch
    image_preprocessed = image_preprocessed.to(device)

    with torch.no_grad():
      image_features = model.encode_image(image_preprocessed)
      image_features /= image_features.norm(dim=-1, keepdim=True)

      text_probs = (100.0 * image_features @ text_features).softmax(dim=-1)
      y_hat = text_probs.argmax(dim=-1).item()

    all_pred.append(y_hat)
    all_y.append(y_int)

    acc = ((np.array(all_pred) - np.array(all_y)) == 0).mean() * 100.
    print(f"Accuracy: {acc:.2f}%")

# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load('ViT-B/32', device)

#New code
# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load('ViT-B/32', device)

# Download the dataset
cifar100 = CIFAR100(root=os.path.expanduser("~/.cache"), download=True, train=False)

# Prepare the inputs
image, class_id = cifar100[3637]
image_input = preprocess(image).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in cifar100.classes]).to(device)

# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%")

#/QuiltNet-B-32 ALREADY EXECUTED--------------------------------------------------------------
from PIL import Image
import requests
import open_clip

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:wisdomik/QuiltNet-B-32') #16
tokenizer = open_clip.get_tokenizer('hf-hub:wisdomik/QuiltNet-B-32')

import torch
from PIL import Image

import os
image_folder = '/content/drive/MyDrive/Unique_BACH_folder'  # Specifica il percorso alla cartella delle immagini
image_files = [f for f in os.listdir(image_folder) if f.endswith(('tif'))]

import pandas as pd

# Leggi il file Excel
df = pd.read_csv('/content/drive/MyDrive/WE/microscopy_ground_truth.csv')


# Inizializza la lista per le coppie
y_vere = []


#-----------------------------------------------------DOVREBBE ESSERCI IL PROBLEMA
# Itera sul DataFrame e crea le coppie
import numpy as np

y_pred = []

# Carica il modello e le trasformazioni
model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:wisdomik/QuiltNet-B-32')

# Prepara il testo
#corispondenza classe t0, t1, t2, t3
texts = ["Breast normal breast tissue", "Breast non-malignant benign tissue", "Breast malignant in-situ carcinoma", "Breast malignant invasive carcinoma"]
#texts = ["Normal breast tissue histology shows well-organized lobules and ducts lined by a double layer of epithelial and myoepithelial cells. The stroma is composed of dense fibrous connective tissue interspersed with adipose tissue. The epithelial cells are uniform in appearance with no signs of atypia or mitotic activity. Myoepithelial cells are present at the periphery of the ducts and lobules, demonstrating the normal architecture. No inflammatory cells or pathological changes are observed. The tissue exhibits a normal distribution of glandular and stromal components, indicative of healthy breast tissue.","benign","in-situ", "invasive"]

tokenizer = open_clip.get_tokenizer('hf-hub:wisdomik/QuiltNet-B-32')


for index, row in df.iterrows():
    coppia = (row[0], row[1])  # Leggi la colonna 0 e la colonna 1 e crea la coppia
    image_file = row[0]
    yt = row[1]
    image_path = os.path.join(image_folder, image_file)
    image = Image.open(image_path)

    # Preprocessa l'immagine usando le trasformazioni di validazione
    image_preprocessed = preprocess_val(image).unsqueeze(0)  # aggiungi una dimensione batch

    # Tokenizza il testo e calcola la lunghezza massima dei token per ogni batch di testi
    tokenized_texts = [tokenizer.encode(text) for text in texts]
    max_len = max(len(tokens) for tokens in tokenized_texts)

    # Padding dinamico dei testi per ogni batch
    padded_texts = [torch.nn.functional.pad(torch.tensor(tokens), (0, max_len - len(tokens)), 'constant', 0) for tokens in tokenized_texts]
    text_inputs = torch.stack(padded_texts)

    # Sposta il modello e gli input su GPU se disponibile
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    image_preprocessed = image_preprocessed.to(device)
    text_inputs = text_inputs.to(device)

    # Passa gli input preprocessati al modello
    with torch.no_grad():
        image_features = model.encode_image(image_preprocessed)

        # Truncate or pad the text inputs to match the expected length of positional embeddings
        expected_length = model.positional_embedding.shape[0] # Get the expected length from the model
        text_inputs = torch.nn.functional.pad(text_inputs, (0, expected_length - text_inputs.shape[1]), 'constant', 0)  # Pad if shorter
        text_inputs = text_inputs[:, :expected_length]  # Truncate if longer

        text_features = model.encode_text(text_inputs)

    # Calcola le similarit√† tra immagine e testi
    logits_per_image = (image_features @ text_features.T).softmax(dim=-1) #----------------------

    max_value, max_index = torch.max(logits_per_image, dim=1)
    posizione = "" #valore di supporto per fare cose


    if (max_index.item() == 3):
      posizione = "Invasive"
    elif (max_index.item() == 2):
      posizione = "InSitu"
    elif (max_index.item() == 1):
      posizione = "Benign"
    else:
      posizione = "Normal"


    #Aggiungere gli elementi ad una lista con soli nomi + labels predette
    y_pred.append((yt, posizione))
    print(y_pred[-1])

label = []
labelp = []
for (y,yp) in y_pred:
  if y== "Invasive":
      label.append(0)
  elif y== "InSitu":
      label.append(1)
  elif y== "Benign":
      label.append(2)
  else:
      label.append(3)
  if yp== "Invasive":
      labelp.append(0)
  elif yp== "InSitu":
      labelp.append(1)
  elif yp== "Benign":
      labelp.append(2)
  else:
      labelp.append(3)

from sklearn.metrics import confusion_matrix
print(confusion_matrix(label, labelp))

from sklearn.metrics import balanced_accuracy_score
print(balanced_accuracy_score(label, labelp))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(label, labelp))

from sklearn.metrics import balanced_accuracy_score
print(balanced_accuracy_score(label, labelp))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(label, labelp))

from sklearn.metrics import balanced_accuracy_score
print(balanced_accuracy_score(label, labelp))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(label, labelp))

from sklearn.metrics import balanced_accuracy_score
print(balanced_accuracy_score(label, labelp))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(label, labelp))

import numpy as np

y_pred = []

# Carica il modello e le trasformazioni
model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:wisdomik/QuiltNet-B-32')

# Prepara il testo
#corispondenza classe t0, t1, t2, t3

texts = ["Breast normal breast tissue","Breast non-malignant benign tissue","Breast malignant in-situ carcinoma", "Breast malignant invasive carcinoma",]
tokenizer = open_clip.get_tokenizer('hf-hub:wisdomik/QuiltNet-B-32')

# Itera su ogni immagine nel folder
for image_file in image_files:
    image_path = os.path.join(image_folder, image_file)
    image = Image.open(image_path)

    # Preprocessa l'immagine usando le trasformazioni di validazione
    image_preprocessed = preprocess_val(image).unsqueeze(0)  # aggiungi una dimensione batch

    # Tokenizza il testo e calcola la lunghezza massima dei token per ogni batch di testi
    tokenized_texts = [tokenizer.encode(text) for text in texts]
    max_len = max(len(tokens) for tokens in tokenized_texts)

    # Padding dinamico dei testi per ogni batch
    padded_texts = [torch.nn.functional.pad(torch.tensor(tokens), (0, max_len - len(tokens)), 'constant', 0) for tokens in tokenized_texts]
    text_inputs = torch.stack(padded_texts)

    # Sposta il modello e gli input su GPU se disponibile
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    image_preprocessed = image_preprocessed.to(device)
    text_inputs = text_inputs.to(device)

    # Passa gli input preprocessati al modello
    with torch.no_grad():
        image_features = model.encode_image(image_preprocessed)

        # Truncate or pad the text inputs to match the expected length of positional embeddings
        expected_length = model.positional_embedding.shape[0] # Get the expected length from the model
        text_inputs = torch.nn.functional.pad(text_inputs, (0, expected_length - text_inputs.shape[1]), 'constant', 0)  # Pad if shorter
        text_inputs = text_inputs[:, :expected_length]  # Truncate if longer

        text_features = model.encode_text(text_inputs)

    # Calcola le similarit√† tra immagine e testi
    logits_per_image = (image_features @ text_features.T).softmax(dim=-1) #----------------------

    max_value, max_index = torch.max(logits_per_image, dim=1)
    posizione = "" #valore di supporto per fare cose


    if (max_index.item() == 3):
      posizione = "Invasive"
    elif (max_index.item() == 2):
      posizione = "InSitu"
    elif (max_index.item() == 1):
      posizione = "Benign"
    else:
      posizione = "Normal"


    #Aggiungere gli elementi ad una lista con soli nomi + labels predette
    y_pred.append((image_file, posizione))






    #Stampa i risultati
    #print(f"Logits per immagine {image_file}:" )
    #print(f"Il valore massimo √® {max_value.item()} e si trova in posizione {max_index.item()}")
    #print("\n")

#Ora stampo il mio vettore di labels prodotteeeeeeeeeeeeee
print("Valori PREDETTI ----------------------------------------------------------------")

for coppia in y_pred:
  if coppia[0] ==  "n001.tif":
    print("Ci sono")
  else:
    print(coppia)
    print("\n")

print("La lunghezza della lista PRED √®:", len(y_pred)) #400 IL MISTERO SI INFITTISCE

"""CODICE CHE LAVORA SULLE LABELS VERE:"""

import pandas as pd

# Leggi il file Excel
df = pd.read_csv('/content/drive/MyDrive/WE/microscopy_ground_truth.csv')


# Inizializza la lista per le coppie
y_vere = []


#-----------------------------------------------------DOVREBBE ESSERCI IL PROBLEMA
# Itera sul DataFrame e crea le coppie

for index, row in df.iterrows():
    coppia = (row[0], row[1])  # Leggi la colonna 0 e la colonna 1 e crea la coppia
    y_vere.append(coppia)  # Aggiungi la coppia alla lista

# Stampa la lista di coppie


print("Valori reali ----------------------------------------------------------------")

for coppia in y_vere:
      print(coppia)
      print("\n")




print("La lunghezza della lista VERE √®:", len(y_vere)) #400


#CODCIE FIXATOOOOOOOOOOOO

"""Sezione del codice in cui, dopo aver verificato la corretta creazione delle due liste andiamo a calcolare l'accuracyyyyyyyy"""

#NON COMPILARE PIUUUUUUUUUUUUU SOLO PER BACKUP DI CODICE

#Andiamo a ripulire il contenuto di y_vere
y_vere = y_vere[:-2]
#verifico la composizione della lista a cui ho inserito l'elemento. Consiglio di fare l'inserimento in una cella separata in maniera tale da aggiungerlo solo quando necessario<
for coppia in y_vere:
    print(coppia)
    print("\n")
#Da qui in poi non modificheremo piu il contenuto di y_vere

#una volta ottenute le due liste, voglio andare ad ordinarmi la lista valori reali

#confornto prima di tutto se le due liste contengono lo stesso numero di elementi
if len(y_pred) == len(y_vere):
    print("Le due liste hanno la stessa lunghezza.")
else:
    print("Le due liste hanno lunghezze diverse.")


#andiamo a debuggare -- - vabbe------------------:)---mandiamo avanti il codice RICORDA DI FIXAREEE
print("La lunghezza della lista PRED √®:", len(y_pred))
print("La lunghezza della lista VERE √®:", len(y_vere))



# Inizializzazione della lista di coppie ordinata
ORDINATA = []


#STACOSANONFUNZIONA
my_counter = 0

# Converti le liste in set per poter fare l'operazione di differenza
set_b = set(y_vere)
set_a = set(y_pred)

# Trova le coppie che sono in B ma non in A
differenza = set_b - set_a

# Converti il risultato in una lista (se necessario)
differenza_lista = list(differenza)

# Stampa la differenza
print("Coppie presenti in B ma non in A:", differenza_lista)

for coppia in y_vere: #Il ciclo viene operato sulla stringa piu corta ed e`per questo che le due stringhe vengono di lunghezza uguale pur non essendolo.!!!!!!!-------------
    ticerco = coppia[0]  # Estrai il primo elemento della coppia corrente di A e salvalo in ticerco
    #print("\n", my_counter) #segnala l'avvenuta aggiunta di un elemento alla lista ordinata
    # Secondo ciclo for per cercare ticerco in tutti i primi elementi delle coppie presenti nella lista di coppie B
    for coppia_b in y_pred:
        # Se trovi ticerco in uno dei primi elementi delle coppie di B
        if ticerco == coppia_b[0]:
            # Aggiungi la coppia di B trovata nella lista ORDINATA
            ORDINATA.append(coppia_b)
            my_counter = my_counter + 1

print("LA LISTA PREDETTA ORDINATA e'", )

for coppia in ORDINATA:
    print(coppia)
    print("\n")

#Quale √® l'elemento che non ha un simile?

print("La lunghezza della lista PRED √®:", len(ORDINATA)) #399
print("La lunghezza della lista VERE √®:", len(y_vere)) #400
#stampa le lunghezze delle due stringhe per
i = 0
index = 0

# fai un for della lunghezza della lista piu lunga, in questo caso predicted labels

for coppia in ORDINATA:
    # Estrai il primo elemento dalla coppia corrente in l1
    elemento_l1 = coppia[0]
    i = index
    for pippo in y_vere:
        # Estrai il primo elemento dalla prima coppia in l2
        elemento_l2 = pippo[0] #perch√® pippoi mi da solo lettere????????!
        #print(elemento_l2) #-------------------------------------------
        # Confronta i primi elementi
        if elemento_l1 == elemento_l2:
            print("Match")
            index = index+1
    if(i == index): #singifica che non √® avvenuto l'aggiornamento
      print("Nessuna corrispondenza per: \n", elemento_l1)

#Per sicurezza stampati la lista A e B in un file di testo per compararle agevolmente
# Definizione della lista di coppie


# Nome del file di output
nome_file_p = '/content/drive/MyDrive/WE/lista_coppie_pred.txt'
nome_file_v = '/content/drive/MyDrive/WE/lista_coppie_vere.txt'

def scrivi_lista_in_file(lista_coppie, nome_file):
    try:
        with open(nome_file, 'w') as file:
            for coppia in lista_coppie:
                file.write(f"{coppia[0]}, {coppia[1]}\n")
        print(f"Lista di coppie scritta in {nome_file}")
    except Exception as e:
        print(f"Errore durante la scrittura nel file: {e}")

scrivi_lista_in_file(ORDINATA, nome_file_p)

scrivi_lista_in_file(y_vere, nome_file_v)

#Ora posso estrarre le labels della prima e della seconda e calcolare l'accuracy

#estraggo le labels reali
reallabels = []
predictedlabels = []

# Iterazione sulla lista di coppie
for coppia in y_vere:
    # Aggiungi il secondo elemento della coppia alla lista reallabels
    reallabels.append(coppia[1])

# Stampare la lista reallabels
print(reallabels)

#estraggo le labels predette----------------------------------------------------------
# Iterazione sulla lista di coppie
for coppia in ORDINATA:
    # Aggiungi il secondo elemento della coppia alla lista reallabels
    predictedlabels.append(coppia[1])

# Stampare la lista reallabels
print(predictedlabels)

len(reallabels), len(predictedlabels)

from sklearn.metrics import accuracy_score

#look at the parameters and their meaning
accuracy_score(reallabels, predictedlabels)